return require 'Q/UTILS/lua/code_gen' {

   declaration = [[

#include "q_incs.h"
${includes}
extern int
${fn}(  
      const ${in1_ctype} * restrict in1,  
      const ${in2_ctype} * restrict in2,  
      uint64_t nR,  
      ${out_ctype} * restrict out 
      ) 
;

   ]],
   definition = [[

#include "_${fn}.h"

__global__
static void
__operation(
      ${in1_ctype} *d_a,
      ${in2_ctype} *d_b, 
      ${out_ctype} *d_c,
      uint64_t size
      )
      {
      uint64_t index = blockIdx.x * blockDim.x + threadIdx.x;
      uint64_t stride = blockDim.x * gridDim.x;
      for (uint64_t i = index; i < size; i += stride) {
        ${c_code_for_operator}
      }
      }

int
${fn}(  
      const ${in1_ctype} * restrict in1,  
      const ${in2_ctype} * restrict in2,  
      uint64_t nR,  
      ${out_ctype} * restrict out 
      )

{ 
  int status = 0;
// TODO #pragma omp parallel for schedule(static, Q_MIN_CHUNK_SIZE_OPENMP)

  // Declare pointers to device array
  ${in1_ctype} *d_in1 = 0;
  ${in2_ctype} *d_in2 = 0;
  ${out_ctype} *d_out = 0;

  // Allocate memory for device arrays
  cudaMalloc (&d_in1, nR * sizeof (int32_t));
  cudaMalloc (&d_in2, nR * sizeof (int16_t));
  cudaMalloc (&d_out, nR * sizeof (int32_t));

  // Copy input from host to device
  cudaMemcopy (d_in1, in1, nR * sizeof(${in1_ctype}), cudaMemcpyHostToDevice);
  cudaMemcopy (d_in2, in2, nR * sizeof(${in2_ctype}), cudaMemcpyHostToDevice);

  // Decide runtime configuration
  uint64_t blockSize = 256;
  uint64_t numBlocks = (nR + 256 - 1) / blockSize;

  // Launch Kernel
  __operation<<<numBlocks, blockSize>>>(d_in1, d_in2, d_out, nR);

  // Copy result from device to host
  cudaMemcpy (out, d_out, nR * sizeof(${out_ctype}), cudaMemcpyDeviceToHost);
  
  return status;
}

   ]]
}
