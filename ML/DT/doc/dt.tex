\startreport{Decision Trees in Q}
\reportauthor{Ramesh Subramonian}

\section{Introduction}

See \cite{Hastie2009}.

\subsection{Notations}

\bi
\item Let \(F = \{f_i\}\) be a table of F4 vectors, representing the features.
\item Let \(g\) be a B1 vector, representing the outcome which we wish to
predict. We will refer to the value 1 as positive and 0 as negative.
\ei

A decision tree is a Lua table where each element identifies
\be
\item a feature
\item a threshold, the default comparison is always \(\leq\).
\item a left decision tree
\item a right decision tree
\ee

\begin{invariant}
\(forall f \in F, f:length() = g:length()\)
\end{invariant}

\begin{figure}
\centering
\fbox{
\begin{minipage}{35cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \=
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
Let \(\alpha\) be minimum benefit required to continue branching \\ 
Let values taken on by goal be 0 (negative), 1 (positive)  \(\Rightarrow n_G = 2\) \\
Let \(n_N =\) number of instances classified as negative \\ 
Let \(n_P =\) number of instances classified as positive \\ 
Let \(T\) be table of vectors, each vector being an attribute \\
Initialize, decision tree \(D = \{\}\) \\
\(F, g\) as described above \\
{\bf function } MakeDecisionTree(D, T, g) \+ \ \\
  \(C = Q.\mathrm{numby}(g, n_G);~n_N = C[0];~n_P = C[1]\) \\ 
  {\bf forall} \(f \in T:~ b(f), s(f) = \mathrm{Benefit}(f, g, n_N, n_P)\) \\ 
  Let \(f'\) be feature with maximum benefit. \\
  {\bf if} \(b(f') \geq \alpha\) {\bf then } \+  \\
    \(x = Q.vsgt(f', s(f'))\) \\
    \(n_R, n  = Q.sum(x) \) \\
    \(F_L = F_R = \{\}\) \\
    {\bf forall} \(f \in F\) {\bf do} \+ \\
      \(Q.reorder(f, x)\) \\ 
      \(F_L = F_L \cup Q.vector(f, 0, n_L)\) \\
      \(F_R = F_L \cup Q.vector(f, n_L, n)\) \- \\
    {\bf endfor} \\
    T.feature = \(f'\) \\
    T.threshold = \(b(f')\) \\
    T.left = \(\{\}\) \\ 
    T.right = \(\{\}\) \\ 
    \(DT(F_L, g_L, T_L)\) \\ 
    \(DT(F_R, g_R, T_R)\) \- \\ 
  {\bf endif} \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{dt_pseudo_code}
\caption{Decision Tree algorithm}
\end{figure}

\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= 
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{Benefit}(f, g, n_N, n_P)\) \+  \\
  \(p_{max} = -\infty\) \\
  \(b_{opt} = \bot\) \\
  \(f', g' = \mathrm{Q.sort2}(f, g, \) ``A\_'' ) ---
  Sort \(f,g\) with \(f\) ascending and \(g\) drag along \\
  \(C = \{\},~C[0] = 0,~C[1] = 0\) --- Counters for goal values \\
  \(i = 0,~ n = n_N + n_P\) \\ 
  {\bf while} \(i < n\) {\bf do} \+ \\
    \(b = f_i\) \\
    \(C[g_i] = C[g_i] + 1\) \\
    \(i = i + 1 \) \\
    {\bf for } ( ; \(i < n\); \(i++\) ) {\bf do} \+ \\
      {\bf if } \(f_i \neq b\) {\bf then } \+ \\
        {\bf break} \- \\
      {\bf endif} \\
      \(C[g_i] = C[g_i] + 1\) \- \\
    {\bf endfor} \\
    \(p = \mathrm{WeightedBenefit}(C[0], C[1], n_N, n_P)\) \\
    {\bf if } \(p > p_{max}\) {\bf then } \+ \\
      \(p_{max} = p \)  \\
      \(b_{opt} = b \) \- \\
    {\bf endif} \- \\
  {\bf endwhile} \\
  {\bf return} \(p_{max}, b_{opt}\) \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_benefit_numeric}
\caption{Benefit Computation (numeric attributes)}
\end{figure}
%%-------------------------------------------
\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{WeightedBenefit}(n_N^L, n_P^L, n_N, n_P)\) \+  \\
  \(n_N^R = n_N - n_N^L\) \\
  \(n_P^R = n_P - n_P^L\) \\
  \(n_R = n_N^R  + n_P^R\) \\ 
  \(n_L = n_N^L  + n_P^L\) \\ 
  {\bf return} \( \frac{n_L}{n} \times XXX + \frac{n_R}{n} \times YYY \) \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_weighted_benefit}
\caption{Weighted Benefit Computation}
\end{figure}
%%-------------------------------------------
\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{Benefit}(f, g, n_N, n_P)\) \+ 

{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_benefit_boolean}
\caption{Benefit Computation (boolean attributes)}
\end{figure}
%%-------------------------------------------

\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{Benefit}(f, g, n_N, n_P)\) \+ 

{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_benefit_categorical}
\caption{Benefit Computation (categorical attributes)}
\end{figure}

\section{Benefit Computation}
It is useful to model this problem as a gambling problem where we are 
sampling with replacement. 
\bi
\item Let there be 100 balls, with 60  being marked Heads and 40
being marked Tails. 
\item Assume that we have to gamble on whether a ball picked at random is Heads
ot Tails. If the ``house'' sets odds to ensure a fair game, then it would offer
a payout of 4/6 if you bet heads and 6/4 if you bet tails. 
\item If you bet heads, then the expected benefit is 
\(\frac{60}{100} \times \frac{4}{6} + 
\frac{40}{100} \times (-1)  = 0\)
\item If you bet tails, then the expected benefit is 
\(\frac{60}{100} \times (-1) + 
\frac{40}{100} \times \frac{6}{4}  = 0\)
\ei

Now, assume that before we place out bet, we are allowed to ask one questionm
which has an answer of left or right. If left, we are told that the ball was
selected from a population of 40 heads and 15 tails. If right, we are told that
the ball was selected from a population of 20 heads and 25 tails. Clearly, we
would bet heads if the answer is left and tails if the answer is right. The
question is: What is the expected benefit for 100 trials?

\(100 ( \times ( \frac{40+15}{100} \times ( 
  \frac{40}{40+15} \times \frac{4}{6}  + 
  \frac{15}{40+15} \times -1 ) ) + 
  ( \frac{20+25}{100} \times ( 
  \frac{20}{20+25} \times -1 + 
  \frac{25}{20+25} \times \frac{6}{4}  ) ) )
  \)
which is \(
 = (40 \times  \frac{4}{6}) -15 - 20 + (25 \times \frac{6}{4} )
 = 29 \frac{1}{6}\)

\bibliographystyle{alpha}
\bibliography{ref}
