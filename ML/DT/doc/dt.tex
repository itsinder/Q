\startreport{Gambling on Decision Trees}
\reportauthor{Ramesh Subramonian}
\newcommand{\extra}{0}

\section{Introduction}

Decision trees are a well understood statistical technique \cite{Hastie2009}.
This paper makes the following contributions:
\be
\item proposes a gambling-inspired metric that allows us to answer
the two fundamental questions associated with building a decision tree:
\be
\item what is the decision to be made at a given node e.g., \(x \leq 0.5?\)
\item when should a node not be split further?
\ee
This is in contrast to the well known metrics (i) gini impurity (ii) information
gain and (iii) variance reduction
\ifthenelse{\equal{extra}{1}}{
\item a more intuitive way to deal with unknown values, both during 
training and testing. This approach is particularly useful when
the fact that the value is null does not have some systemic bias.
Section~\ref{unknowns}
}{}
\item what ``decision'' does the decision tree really make? For example, if the
evaluation of the tree leaves us at a node which had 
\(n_H\) instances of class ``Heads'' and 
\(n_T\) instances of class ``Tails'' at training time, what statement are we
making?
\ifthenelse{\equal{extra}{1}}{
\item when using several decision trees to create a random forest, using
principal component analysis to prune correlated trees
}{}
\item weighting the results of the different decision trees based 
on their accuracy, instead of simply averaging the results.
\ee

We believe that these changes to the way decision trees are created and their
results combined is particularly relevant to the case where relative
confidences are involved. This is often the case when multiple candidates are
competing for a limited resource, like user attention. 

For example, supposed we have 2 classifiers, \(C_A, C_B\), one for product A and one for
product B.  We need \(C_A, C_B\) to return \(p_A, p_B\), where \(p_A\) is
\(C_A\)'s estimate of how likely the user is to select product A. In order to
make the best business decision, we would like \(p_A, p_B\) to be as asccurate
as possible.

\subsection{Conventions}
Throughout the paper, we use the data sets listed in Table~\ref{tbl_data_sets}
for our experiments. 
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|} \hline \hline
{\bf Data Set Name } & {\bf Rows} & {\bf Columns} & {\bf Location} \\ \hline 
Breast Cancer & X & X & \\ \hline
Titanic & X & X & \\ \hline
\ldots & X & X & \\ \hline
\end{tabular}
\label{tbl_data_sets}
\caption{Data Sets used for experimentation}
\end{table}

\subsection{Notations}

\bi
\item Let \(F = \{f_i\}\) be a table of vectors, representing the features.
\item Let \(g\) be a vector, representing the {\bf goal} or 
outcome which we wish to
predict. We assume the goal can be either Heads, represented by the integer value 1 or Tails, represented as 0.
\item Restrictions on vectors (for a Q implementation)
\be
\item lengths of all vectors must be the same. 
\item qtype of all feature vectors should be F4. 
% TODO Could be F*
Note that boolean features are
modeled as having type F4 and the comparison is always \(\leq 0\), so that the
left branch has instances where the feature value is false and the
right branch has instances where the feature value is true.
% TODO SV to be supported in future
\item goal vector should be I4
% TODO: could be I*
\ee
\ei

A decision tree is a Lua table where each element identifies
\be
\item a feature e.g., age
\item the comparison to be made e.g., age \(\leq 40\). Currently, the comparison
is {\bf always} \(\leq\)
\item a left decision tree, which may be null
\item a right decision tree, which may be null
\ee

\begin{figure}
\centering
\fbox{
\begin{minipage}{35cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \=
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
Let \(\alpha\) be minimum benefit required to continue branching \\
Let \(n_G = 2\) be the number of values of the goal attribute \\
Let \(n_T =\) number of instances classified as tails \\
Let \(n_H =\) number of instances classified as heads \\
Let \(T\) be table of vectors, each vector being an attribute \\
Initialize, decision tree \(D = \{\}\) \\
\(F, g\) as described above \\
{\bf function } MakeDecisionTree(D, T, g) \+ \ \\
  \(C = Q.\mathrm{numby}(g, n_G);~n_T = C[0];~n_H = C[1]\) \\
  {\bf forall} \(f \in T:~ b(f), s(f) = \mathrm{Benefit}(f, g, n_T, n_H)\) \\
  Let \(f'\) be feature with maximum benefit. \\
  {\bf if} \(b(f') < \alpha\) {\bf then } \+  \\
    {\bf return} \- \\
  {\bf endif} \\
    \(x = \mathrm{Q.vsgt}(f', s(f'))\) \\
    \(T_L = T_R = \{\}\) \\
    {\bf forall} \(f \in F\) {\bf do} \+ \\
      \(T_L = T_L \cup \mathrm{Q.where}(f, x)\) \\ 
      \(T_R = T_R \cup \mathrm{Q.where}(f, \mathrm{Q.not}(x))\) \\ 
    {\bf endfor} \\
    D.feature = \(f'\) \\
    D.threshold = \(b(f')\) \\
    D.left = \(\{\}\) \\
    D.right = \(\{\}\) \\
    \(DT(D.left, g_L, T_L)\) \\
    \(DT(D.right, F_R, g_R, T_R)\) \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{dt_pseudo_code}
\caption{Decision Tree algorithm}
\end{figure}

\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= 
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{Benefit}(f, g, n_T, n_H)\) \+  \\
  \(f', h^0, h^1 = CountIntervals(f, g)\) \\
  \(b = \mathrm{Q.wtbnft}(h^0, h^1, n_T, n_H)\) (Table~\ref{algo_weighted_benefit}) \\
  \(b', \_, i = \mathrm{Q.max}(b)\) \\
  {\bf return} \(b', f'[i]\)  \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_benefit_numeric}
\caption{Benefit Computation (numeric attributes)}
\end{figure}
%%-------------------------------------------
\begin{table}
\centering
\begin{tabular}{|l|l|l|} \\ \hline \hline
{\bf Variable} & {\bf Formula} & {\bf Explanation} \\ \hline \hline
\(n_T^L\)  & & number of tails on left \\ \hline
\(n_H^L\)  & & number of heads on left \\ \hline
\(n_T^R\)  & & number of tails on right \\ \hline
\(n_H^R\) & & number of heads on right \\ \hline

\(n_L\)    & \(n_T^L + n_H^L \) & number on left \\ \hline
\(n_R\)    & \(n_T^R + n_H^R \) & number on right \\ \hline

\(n\)      & \(n_T + n_H \) & total number \\ \hline

\(w_L \) & \( n^L/n\)  & weight on left \\ \hline
\(w_R \) & \( n^R/n\)  & weight on left \\ \hline
\hline
\(o_H \) & \( n_T/n_H\) & odds for heads \\ \hline
\(o_T \) & \( n_H/n_T\) & odds for tails \\ \hline
\hline

\(p_H^L \) & \( n_H^L/n_L\) & probability of heads on left \\ \hline
\(p_H^R \) & \( n_H^R/n_R\) & probability of heads on right \\ \hline
\(p_T^L \) & \( n_T^L/n_L\) & probability of tails on left \\ \hline
\(p_T^R \) & \( n_T^R/n_R\) & probability of tails on right \\ \hline
\hline

\(b_H^L\) &  \(o_H^L \times p_H^L + (-1)  \times p_T^L\) &
            benefit of betting heads on left \\ \hline
\(b_T^L\) & \(o_T^L \times p_T^L + (-1)  \times p^H_L \) &
            benefit of betting tails on left \\ \hline

\(b_H^R\) & \(o_H^R \times p_H^R + (-1)  \times p_T^R \) &
            benefit of betting heads on right \\ \hline
\(b_T^R\) & \(o_T^R \times p_T^R + (-1)  \times p_H^R \) &
            benefit of betting tails on right \\ \hline
\hline

\(b_L\) & \( \mathrm{max}(b_H^L, b_T^L)\) & benefit on left \\ \hline
\(b_R\) & \( \mathrm{max}(b_H^R, b_T^R)\) & benefit on right \\ \hline
\hline

\(b\) &  \(b_L \times w_L + b_R \times w_R\) & weighted benefit \\ \hline
\end{tabular}
\label{algo_weighted_benefit}
\caption{Weighted Benefit Computation}
\end{table}
%%-------------------------------------------
\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{CountIntervals}(f, g)\) \+  \\
{\bf Inputs} \+ \\  
Vector \(f\) of length \(n\) qtype F4 \\
Vector \(g\) of length \(n\) qtype I4 with values 0, 1 \- \\
{\bf Outputs} \+ \\
Vector \(f'\) of length \(n'\) type F4 \\
Vector \(h^0\) of length \(n'\) type I4  \\
Vector \(h^1\) of length \(n'\) type I4  \- \\
\(f'\) is the unique values of \(f\), sorted ascending \\
Let \(\delta(x)\) = 1 if \(x\) is true and 0 otherwise \\
For \(k = 0, 1\), compute \(h^k_j = \sum_{i=1}^{i=n} \delta(g_i = k \wedge f_i \leq f_j)\) \- \\

{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{count_intervals}
\caption{Count Intervals}
\end{figure}
%%-------------------------------------------

\section{Algorithm}

Since decision trees have been well documented, we focus our attention on the
salient characteristics of our approach which are
\be
\item how does one decide on the comparison to be performed at a node?
\item how does one decide when to not split a node further, assuming that asplit
is possible at all.
\ee

Given the simple recursive nature of decision trees, we focus our 
decision on what is done at a single node. Without loss of generality, 
let this be the root node. At this point, we know the number of elements
classified as Heads and Tails, \(n_H, n_T\) respectively. This allows us to set
the odds of betting on heads or tails. At each node, we are asked to a single
question. 
\bi
\item For a numeric attribute, it is of the form \(x \leq y\)
\item For a boolean attribute, it is of the form \(x == y\), where 
\(y\) can be true or false
%% TODO LATER
%% \item For an unordered categorical attribute that can take on values in \(X\),
%% it is of the form \(x \in y \subset X\)
\ei
We iterate over all possible values of \(y\) for each attribute and find the
attribute and decision that produces the maximum benefit. If this benefit
exceeds a pre-defined threshold, we split the data into two, a left branch and a
right branch and proceed recursively. Else, the expansion of the decision tree
stops at this point.
Figure~\ref{XXX} describes the algorithm at this level. For clarity of 
exposition, we only describe the treatment of numeric attributes.


\subsection{Q Operators Used}
The following Q operators are used.
\be
\item Q.not(x). \(x\) is a boolean Vector. output is y, a boolean vector, the
negation of \(x\)
\item Q.vsgt(x, s). \(x\) is a Vector, \(s\) is a scalar. Output is \(y\) a
boolean vector such that \(x_i \geq s \Rightarrow y_i = \mathrm{true}\) and
\(y_i\) is false otherwise
\item Q.sum(x). \(x\) is a Vector. output is a scalar
\item Q.numby(x, n). \(x\) is a Vector, \(n\) is a positive integer 
such that \(x_i \in [0, n-1]\). output is a vector \(y\) such that \(y_i\) is
the number of elements of \(x\) that have value \(i\). Note that
\(y:mathrm{length} == n\)
\ee

\subsection{Benefit Computation}
It is useful to model this problem as a gambling problem where we are 
sampling with replacement. 
\bi
\item Let there be 100 balls, with 60  being marked Heads and 40
being marked Tails. 
\item Assume that we have to gamble on whether a ball picked at random is Heads
ot Tails. If the ``house'' sets odds to ensure a fair game, then it would offer
a payout of 4/6 if you bet heads and 6/4 if you bet tails. 
\item If you bet heads, then the expected benefit is 
\(\frac{60}{100} \times \frac{4}{6} + 
\frac{40}{100} \times (-1)  = 0\)
\item If you bet tails, then the expected benefit is 
\(\frac{60}{100} \times (-1) + 
\frac{40}{100} \times \frac{6}{4}  = 0\)
\ei

Now, assume that before we place out bet, we are allowed to ask one question
which has an answer of left or right. If left, we are told that the ball was
selected from a population of 40 heads and 15 tails. If right, we are told that
the ball was selected from a population of 20 heads and 25 tails. Clearly, we
would bet heads if the answer is left and tails if the answer is right. The
question is: What is the expected benefit for 100 trials? 
The answer is \(29 1/6\) and not 0 as before, calculated as follows.

\(100 ( \times ( \frac{40+15}{100} \times ( 
  \frac{40}{40+15} \times \frac{4}{6}  + 
  \frac{15}{40+15} \times -1 ) ) + 
  ( \frac{20+25}{100} \times ( 
  \frac{20}{20+25} \times -1 + 
  \frac{25}{20+25} \times \frac{6}{4}  ) ) )
  \)
which is \(
 = (40 \times  \frac{4}{6}) -15 - 20 + (25 \times \frac{6}{4} )
 = 29 \frac{1}{6}\)

\ifthenelse{\equal{extra}{1}}{
\subsubsection{Categorical Attributes}
We deal with categorical attributes as follows. Let \(X\) be a categorical
attribute that takes on values \(\{x_1, x_2, \ldots x_M\}\), where \(M\) is
typically small. We can assume less than 100 values.

Let \(Y = \{(x_i, g_i)\}\) be the data set, where \(i = 1, \ldots N\). 
By this we ean
that the \(i^{th}\) point has value \(x_i \in X\) and its goal attribute has
value \(g_i \in \{0, 1\}\). 
\(Y\) may have other features but for now we are concentrating on \(X\).

Create a table, \(Z\), of vectors \(N, P, \rho\) of length \(M\) as follows.  
\begin{enumerate}
\item 
\(n_j = \sum_{i=1}^{i=N} z(i, j)\) where 
\(z_j = 1 \) if \(x_i = x_j \) and \(g_i = 0\)
\item 
\(p_j = \sum_{i=1}^{i=N} z(i, j)\) where 
\(z_j = 1 \) if \(x_i = x_j \) and \(g_i = 1\)
\item 
\(rho_j = \frac{p_j}{p_j+n_j}\)
\end{enumerate}
Sort \(Z\) on \(rho\). Create vectors, \(N', P'\) of length \(M\) as follows
\be
\item 
\ee
}{}

\newpage
\section{Evaluating a decision tree}
In addition to the standard metrics, such as classification accuracy, we
evaluate decision trees as follows. Every element of the data set is 
fed into the decision tree, thereby
assigning each one to a leaf. Now, for each leaf, we have four numbers.
\be
\item \(n^H_1, n^T_1\) which is the number of testing samples assigned 
to the leaf that were labeled heads and tails respectively. The ``weight'' of a
leaf is \(n^H_1 + n^T_1\)
\item \(n^H_0, n^T_0\) same as above but for the training samples
\ee

These numbers are used follows. Continuing our gambling metaphor,
the house is given only \(n^H_0, n^T_0\) to set the odds, whereas the gambler
has access to all four.
The ``true'' probability of heads, \(\rho\), is assumed to be 
\(\frac{n^H_1}{n^H_1+ n^T_1}\). 

Consider three examples, in all of which \(n^H_0, n^T_0 = 60, 40\). 
The benefit, \(b\) that the gambler can expect to make per trial varies
depending on \(rho\) as follows.
\be
\item \(\rho = 0.8\). The adversary would bet heads.
\(b = 0.8 \times \frac{40}{60} + 0.2 \times -1 = \frac{1}{3}\)
\item \(\rho = 0.6\). If the adversary bet heads, 
\(b = 0.6 \times \frac{40}{60} + 0.4 \times -1 = 0\)
If the adversary bet tails, 
\(b = 0.6 \times -1 + 0.4 \times \frac{60}{40} = 0\).
\item \(\rho = 0.4\). The adversary would bet tails.
\(b = 0.4 \times -1 + 0.6 \times \frac{60}{40} = \frac{1}{2}\) 
\ee

The weighted benefit of a leaf is simply \(w \times b\).
The ``cost'' of the decision tree is sum over the 
weighted benefit, normalized by the weights of all leaves 
\(= \frac{\sum_l w_l \times b_l}{\sum_l w_l}\)

\subsection{When not to split?}

In Figure~\ref{XXX}, we stated that we stop splitting when the weighted benefit
is less than a threshold of \(\alpha\). We are now in a position to describe how
\(\alpha\) is determined. We search the space of possible values of \(\alpha =
(0, 1)\), evaluating the ``cost'' for each \(\alpha\). A sample on the breast
cancer data set (reference TODO) is in
Figure~\ref{fig_breast_cancer_cost_versus_alpha}.

We pick the \(alpha\) which produces the minimum cost. For a variety of data
sets, the shape of the curve is similar to 
Figure~\ref{fig_breast_cancer_cost_versus_alpha} but we have not been able to
establish an analytical proof that the function is U-shaped.

%% TODO \input{unknown}
\bibliographystyle{alpha}
\bibliography{../../../DOC/ref.bib}
