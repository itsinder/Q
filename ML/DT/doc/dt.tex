\startreport{Gambling on Decision Trees}
\reportauthor{Ramesh Subramonian}
\newcommand{\extra}{0}

\section{Introduction}

Decision trees are a well understood statistical technique \cite{Hastie2009}.
This paper makes the following contributions:
\be
\item proposes a gambling-inspired metric that allows us to answer
the fundamental question faced when building a tree: 
what is the decision to be made at a given node e.g., \(x \leq 0.5?\)
This is in contrast to the well known metrics (i) gini impurity (ii) information
gain and (iii) variance reduction
\item continuing the gambling metaphor, proposes a ``payout'' metric which
measures the quality of the decision tree. This is in contrast to other means of
evaluating the tree such as precision, recall, accuracy, f1-score, \ldots
\ee

We believe that these changes to the way decision trees are created and
evaluated is particularly relevant to the case where relative
confidences are involved. This is often the case when multiple candidates are
competing for a limited resource, like user attention. 
For example, supposed we have 2 classifiers, \(C_A, C_B\), one for product A and one for
product B.  We need \(C_A, C_B\) to return \(p_A, p_B\), where \(p_A\) is
\(C_A\)'s estimate of how likely the user is to select product A. In order to
make the best business decision, we would like \(p_A, p_B\) to be as asccurate
as possible.

\subsection{Conventions}
Throughout the paper, we use the data sets listed in Table~\ref{tbl_data_sets}
for our experiments. 
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|} \hline \hline
{\bf Data Set Name } & {\bf Rows} & {\bf Columns} & {\bf Location} \\ \hline 
Breast Cancer & X & X & \\ \hline
Titanic & X & X & \\ \hline
\ldots & X & X & \\ \hline
\end{tabular}
\label{tbl_data_sets}
\caption{Data Sets used for experimentation}
\end{table}

\subsection{Notations}

\bi
\item Let \(F = \{f_i\}\) be a table of vectors, representing the features.
\item Let \(g\) be a vector, representing the {\bf goal} or 
outcome which we wish to
predict. We assume the goal can be either Heads, represented by the integer value 1 or Tails, represented as 0.
\item Restrictions on vectors (for a Q implementation)
\be
\item lengths of all vectors must be the same. 
\item qtype of all feature vectors should be F4. 
% TODO Could be F*
Note that boolean features are
modeled as having type F4 and the comparison is always \(\leq 0\), so that the
left branch has instances where the feature value is false and the
right branch has instances where the feature value is true.
% TODO SV to be supported in future
\item goal vector should be I4
% TODO: could be I*
\ee
\ei

A decision tree is a Lua table where each element identifies
\be
\item a feature e.g., age
\item the comparison to be made e.g., age \(\leq 40\). Currently, the comparison
is {\bf always} \(\leq\)
\item a left decision tree, which may be null
\item a right decision tree, which may be null
\ee

\begin{figure}
\centering
\fbox{
\begin{minipage}{35cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \=
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
Let \(\alpha\) be minimum benefit required to continue branching \\
Let \(n_G = 2\) be the number of values of the goal attribute \\
Let \(n_T =\) number of instances classified as tails \\
Let \(n_H =\) number of instances classified as heads \\
Let \(T\) be table of vectors, each vector being an attribute \\
Initialize, decision tree \(D = \{\}\) \\
\(F, g\) as described above \\
{\bf function } MakeDecisionTree(D, T, g) \+ \ \\
  \(C = Q.\mathrm{numby}(g, n_G);~n_T = C[0];~n_H = C[1]\) \\
  {\bf forall} \(f \in T:~ b(f), s(f) = \mathrm{Benefit}(f, g, n_T, n_H)\) \\
  Let \(f'\) be feature with maximum benefit. \\
  {\bf if} \(b(f') < \alpha\) {\bf then } \+  \\
    {\bf return} \- \\
  {\bf endif} \\
    \(x = \mathrm{Q.vsgt}(f', s(f'))\) \\
    \(T_L = T_R = \{\}\) \\
    {\bf forall} \(f \in F\) {\bf do} \+ \\
      \(f_L = Q.\mathrm{where}(f, x)\) \\
      \(f_R = Q.\mathrm{where}(f, \mathrm{Q.not}(x))\) \\
      {\bf if } \(Q.max(f_L) > Q.min(f_L)\) {\bf then} \+ \\
        \(T_L = T_L \cup f_L \) \- \\
      {\bf endif} \\
      {\bf if } \(Q.max(f_R) > Q.min(f_R)\) {\bf then} \+ \\
        \(T_R = T_R \cup f_R \) \- \\
      {\bf endif} \- \\
    {\bf endfor} \\
    D.feature = \(f'\) \\
    D.threshold = \(b(f')\) \\
    D.left = \(\{\}\) \\
    D.right = \(\{\}\) \\
    \(DT(D.left, g_L, T_L)\) \\
    \(DT(D.right, F_R, g_R, T_R)\) \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{dt_pseudo_code}
\caption{Decision Tree algorithm}
\end{figure}

\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= 
                \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{Benefit}(f, g, n_T, n_H)\) \+  \\
  \(f', h^0, h^1 = CountIntervals(f, g)\) \\
  \(b = \mathrm{Q.wtbnft}(h^0, h^1, n_T, n_H)\) (Table~\ref{algo_weighted_benefit}) \\
  \(b', \_, i = \mathrm{Q.max}(b)\) \\
  {\bf return} \(b', f'[i]\)  \- \\
{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{compute_benefit_numeric}
\caption{Benefit Computation (numeric attributes)}
\end{figure}
%%-------------------------------------------
\begin{table}
\centering
\begin{tabular}{|l|l|l|} \\ \hline \hline
{\bf Variable} & {\bf Formula} & {\bf Explanation} \\ \hline \hline
\(n_T^L\)  & & number of tails on left \\ \hline
\(n_H^L\)  & & number of heads on left \\ \hline
\(n_T^R\)  & & number of tails on right \\ \hline
\(n_H^R\) & & number of heads on right \\ \hline

\(n_L\)    & \(n_T^L + n_H^L \) & number on left \\ \hline
\(n_R\)    & \(n_T^R + n_H^R \) & number on right \\ \hline

\(n\)      & \(n_T + n_H \) & total number \\ \hline

\(w_L \) & \( n^L/n\)  & weight on left \\ \hline
\(w_R \) & \( n^R/n\)  & weight on left \\ \hline
\hline
\(o_H \) & \( n_T/n_H\) & odds for heads \\ \hline
\(o_T \) & \( n_H/n_T\) & odds for tails \\ \hline
\hline

\(p_H^L \) & \( n_H^L/n_L\) & probability of heads on left \\ \hline
\(p_H^R \) & \( n_H^R/n_R\) & probability of heads on right \\ \hline
\(p_T^L \) & \( n_T^L/n_L\) & probability of tails on left \\ \hline
\(p_T^R \) & \( n_T^R/n_R\) & probability of tails on right \\ \hline
\hline

\(b_H^L\) &  \(o_H^L \times p_H^L + (-1)  \times p_T^L\) &
            benefit of betting heads on left \\ \hline
\(b_T^L\) & \(o_T^L \times p_T^L + (-1)  \times p^H_L \) &
            benefit of betting tails on left \\ \hline

\(b_H^R\) & \(o_H^R \times p_H^R + (-1)  \times p_T^R \) &
            benefit of betting heads on right \\ \hline
\(b_T^R\) & \(o_T^R \times p_T^R + (-1)  \times p_H^R \) &
            benefit of betting tails on right \\ \hline
\hline

\(b_L\) & \( \mathrm{max}(b_H^L, b_T^L)\) & benefit on left \\ \hline
\(b_R\) & \( \mathrm{max}(b_H^R, b_T^R)\) & benefit on right \\ \hline
\hline

\(b\) &  \(b_L \times w_L + b_R \times w_R\) & weighted benefit \\ \hline
\end{tabular}
\label{algo_weighted_benefit}
\caption{Weighted Benefit Computation}
\end{table}
%%-------------------------------------------
\begin{figure}
\centering
\fbox{
\begin{minipage}{15cm}
\begin{tabbing} \hspace*{0.25in} \=  \hspace*{0.25in} \= \kill
{\bf function } \(\mathrm{CountIntervals}(f, g)\) \+  \\
{\bf Inputs} \+ \\  
Vector \(f\) of length \(n\) qtype F4 \\
Vector \(g\) of length \(n\) qtype I4 with values 0, 1 \- \\
{\bf Outputs} \+ \\
Vector \(f'\) of length \(n'\) type F4 \\
Vector \(h^0\) of length \(n'\) type I4  \\
Vector \(h^1\) of length \(n'\) type I4  \- \\
\(f'\) is the unique values of \(f\), sorted ascending \\
Let \(\delta(x)\) = 1 if \(x\) is true and 0 otherwise \\
For \(k = 0, 1\), compute \(h^k_j = \sum_{i=1}^{i=n} \delta(g_i = k \wedge f_i \leq f_j)\) \- \\

{\bf end} 
\end{tabbing}
\end{minipage}
}
\label{count_intervals}
\caption{Count Intervals}
\end{figure}
%%-------------------------------------------

\section{Algorithm}

Since decision trees have been well documented, we focus our attention on the
salient characteristics of our approach which are
\be
\item how does one decide on the comparison to be performed at a node?
\item how does one decide when to not split a node further, assuming that asplit
is possible at all.
\ee

Given the simple recursive nature of decision trees, we focus our 
decision on what is done at a single node. Without loss of generality, 
let this be the root node. At this point, we know the number of elements
classified as Heads and Tails, \(n_H, n_T\) respectively. This allows us to set
the odds of betting on heads or tails. At each node, we are asked to a single
question. 
\bi
\item For a numeric attribute, it is of the form \(x \leq y\)
\item For a boolean attribute, it is of the form \(x == y\), where 
\(y\) can be true or false
%% TODO LATER
%% \item For an unordered categorical attribute that can take on values in \(X\),
%% it is of the form \(x \in y \subset X\)
\ei
We iterate over all possible values of \(y\) for each attribute and find the
attribute and decision that produces the maximum benefit. If this benefit
exceeds a pre-defined threshold, we split the data into two, a left branch and a
right branch and proceed recursively. Else, the expansion of the decision tree
stops at this point.
Figure~\ref{dt_pseudo_code} describes the algorithm at this level. For clarity of 
exposition, we only describe the treatment of numeric attributes.


\subsection{Q Operators Used}
The Q operators used in Table~\ref{tbl_q_op}.
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|} \hline \hline
{\bf Operator} & {\bf Input} & {\bf Output} & {\bf Explanation} \\ \hline \hline
y = not(x) & x B1 Vector & y B1 Vector & \(y_i = ~x_i\) \\ \hline
y = vsgt(x, s) & x Vector, s Scalar & \(y\) B1 Vector & 
\(x_i \geq s \Rightarrow y_i = \) true; else, \(y_i = \)  false \\ \hline
y = sum(x) & x Vector & y Scalar & \(y = \sum_i x_i\)  \\ \hline
numby(x, n) & x Vector, n number, 
% \(x_i \in [0, n-1]\)
&
y Vector & \(y_i = \) number of elements of \(x\) that have value \(i\) \\ \hline
% . Note that \(y:\mathrm{length} == n\)
\hline
\end{tabular}
\caption{Q Operators Used}
\label{tbl_q_op}
\end{table}

\subsection{Benefit Computation}
It is useful to model this problem as a gambling problem where we are 
sampling with replacement. 
\bi
\item Let there be 100 balls, with 60  being marked Heads and 40
being marked Tails. 
\item Assume that we have to gamble on whether a ball picked at random is Heads
ot Tails. If the ``house'' sets odds to ensure a fair game, then it would offer
a payout of 4/6 if you bet heads and 6/4 if you bet tails. 
\item If you bet heads, then the expected benefit is 
\(\frac{60}{100} \times \frac{4}{6} + 
\frac{40}{100} \times (-1)  = 0\)
\item If you bet tails, then the expected benefit is 
\(\frac{60}{100} \times (-1) + 
\frac{40}{100} \times \frac{6}{4}  = 0\)
\ei

Now, assume that before we place out bet, we are allowed to ask one question
which has an answer of left or right. If left, we are told that the ball was
selected from a population of 40 heads and 15 tails. If right, we are told that
the ball was selected from a population of 20 heads and 25 tails. Clearly, we
would bet heads if the answer is left and tails if the answer is right. The
question is: What is the expected benefit for 100 trials? 
The answer is \(29 \frac{1}{6}\) and not 0 as before, calculated as follows.

\(100 ( \times ( \frac{40+15}{100} \times ( 
  \frac{40}{40+15} \times \frac{4}{6}  + 
  \frac{15}{40+15} \times -1 ) ) + 
  ( \frac{20+25}{100} \times ( 
  \frac{20}{20+25} \times -1 + 
  \frac{25}{20+25} \times \frac{6}{4}  ) ) )
  \)
which is \(
 = (40 \times  \frac{4}{6}) -15 - 20 + (25 \times \frac{6}{4} )
 = 29 \frac{1}{6}\)

\ifthenelse{\equal{extra}{1}}{
\subsubsection{Categorical Attributes}
We deal with categorical attributes as follows. Let \(X\) be a categorical
attribute that takes on values \(\{x_1, x_2, \ldots x_M\}\), where \(M\) is
typically small. We can assume less than 100 values.

Let \(Y = \{(x_i, g_i)\}\) be the data set, where \(i = 1, \ldots N\). 
By this we ean
that the \(i^{th}\) point has value \(x_i \in X\) and its goal attribute has
value \(g_i \in \{0, 1\}\). 
\(Y\) may have other features but for now we are concentrating on \(X\).

Create a table, \(Z\), of vectors \(N, P, \rho\) of length \(M\) as follows.  
\begin{enumerate}
\item 
\(n_j = \sum_{i=1}^{i=N} z(i, j)\) where 
\(z_j = 1 \) if \(x_i = x_j \) and \(g_i = 0\)
\item 
\(p_j = \sum_{i=1}^{i=N} z(i, j)\) where 
\(z_j = 1 \) if \(x_i = x_j \) and \(g_i = 1\)
\item 
\(rho_j = \frac{p_j}{p_j+n_j}\)
\end{enumerate}
Sort \(Z\) on \(rho\). Create vectors, \(N', P'\) of length \(M\) as follows
\be
\item 
\ee
}{}

\newpage
\section{Evaluating a decision tree}
\label{payout}
In addition to the standard metrics, such as classification accuracy, we
evaluate decision trees using the ``payout'' metric.
Table~\ref{dt_eval_code} describes what happens at a particular leaf of the decision tree. Let's explain the intuition with an example. Consider a leaf which had 
\bi
\item 20 Heads and 30 Tails in the training set and 
\item 10 Heads and 40 Tails in the testing set 
\ei
When a head is presented to this leaf, the prediction is Tails. We award the
algorithm a credit of 2/5 and a debit of 3/5.
Similarly, when a tail is presented to this leaf, the prediction is Tails. We award the
algorithm a credit of 3/5 and a debit of 2/5. We average the payout for every
testing instance and that is construed to be the ``benefit'' of this decision tree.


Let \(b_x, w_x\) be the values for leaf \(x\). Then, the total payout of the tree is \(\frac{\sum_x b_x}{\sum_x w_x}\)

\begin{table}[hbtp]
\centering
\begin{tabular}{|l|l|l|} \hline \hline
\(n^H_1\) & number of heads in test data & \\ \hline
\(n^T_1\) & number of tails in test data & \\ \hline
\(n^H_0\) & number of heads in train data&  \\ \hline
\(n^T_0\) & number of tails in train data&  \\ \hline
%%
\(p^H\) & probability of heads at leaf & \(n^H_0 / (n^H_0 + n^T_0)\) \\ \hline
\(p^T\) & probability of tails at leaf & \(n^T_0 / (n^H_0 + n^T_0)\) \\ \hline
%%
\(b^H\) & payout for heads in test data & \(n^H_1 \times ( p^H - p^T )\)  \\ \hline
\(b^T\) & payout for tails in test data & \(n^T_1 \times ( p^T - p^H )\)  \\ \hline
%%
\(b\) & payout for test data & \(b_H + b_T\) \\ \hline

\(w\) & number of test instances & \(n^H_1 + n^T_1\) \\ \hline
\(\bar{b}\) & payout per instance & \(b/w\) \\ \hline
\hline
\end{tabular}
\label{dt_eval_code}
\caption{How a Leaf of a Decision Tree is evaluated}
\end{table}

\section{When to stop splitting?}

In Figure~\ref{dt_pseudo_code}, we stated that we stop splitting when the weighted benefit
is less than a threshold of \(\alpha\). We are now in a position to describe how
\(\alpha\) is determined. We search the space of possible values of \(\alpha =
(0, 1)\), evaluating the ``payout'' (Section~\ref{payout} for each \(\alpha\). 
We pick the \(\alpha\) which produces the maximum payout. 
%% A sample on the breast cancer data set (reference TODO) is in
%% Figure~\ref{fig_breast_cancer_cost_versus_alpha}.

%% TODO \input{unknown}
\bibliographystyle{alpha}
\bibliography{../../../DOC/ref.bib}
