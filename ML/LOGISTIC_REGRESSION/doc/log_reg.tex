\documentclass[12pt,timesnewroman,letterpaper]{article}
\input{ramesh_abbreviations}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{fancyheadings}
\pagestyle{fancy}
\usepackage{pmc}
\usepackage{graphicx}
\setlength\textwidth{6.5in}
\setlength\textheight{8.5in}
\begin{document}
\title{Logistic Regression in Q}
\author{ Ramesh Subramonian }
\maketitle
\thispagestyle{fancy}
\lfoot{{\small Data Analytics Team}}
\cfoot{}
\rfoot{{\small \thepage}}

\section{Introduction}

\TBC

\section{Data Structures}

\bi
\item \(X\) is an \(N \times M\) matrix containing the input data.
  \(X_{i, j}\) is value of \(j^{th}\) attribute of \(i^{th}\)
  instance. \(X\) is stored as \(M\) columns, where \(X_j\) is
  observations for attribute \(j\)

Note: We must add an additional attribute which is the constant 1. Really, what
we're solving for is a solution to $Y = \beta_0 + \sum_{j = 1}^N
X_j \beta_j$, but to express this as one matrix expression we add a column of 1s to
$X$ and solve $Y = X^T\beta$.

\item \(y\) is an \(N \times 1\) classification vector. \(y_i\) is
  classification of instance \(i\) and can be 1 or 0.
\item \(\beta\) is an \(M \times 1\) coefficient vector. \(\beta_j\)
  is coefficient for attribute \(j\). 
\item \(\beta^{\mathrm new}\) is an \(M \times 1\) vector, which are the new
  coefficients that we solve for in each iteration
\item \(A\) is an \(M \times M\) matrix. Since it is symmetric, we can skip
  computing the lower diagonal elements.
\item \(W\) is actually an \(N \times N\) matrix. However, since
  off-diagonal elements are zero, we can represent is as an \(N \times
  1\) vector.   When used as a vector, we will use lower case \(w\). When used
  as a matrix, we will use upper case \(W\). Note that \(W_{i, i} = w_i\) and
  that \( i \neq j \Rightarrow W_{i,j} = 0\)
\item \(b\) is an \(M \times 1\) vector, \(X^T W ( X \beta + W^{-1}(y - p) )\)
  \ei

We start with an initial guess for \(\beta\) and then calculate better
approximations using Equation~\ref{eqn1} until convergence.
\begin{equation}
\label{eqn1}
(X^T W X) \times \beta ^{new} = X^T W ( X \beta + W^{-1}(y - p) )
\end{equation}
However we can simplify this expression by subtracting $X^TWX\beta$ from both
sides to get
\begin{equation}
\label{eqn2}
(X^T W X) \times (\beta ^{new} - \beta) = X^T(y - p)
\end{equation}
and when we find $\beta^{new} - \beta$, we can add $\beta$ to get $\beta^{new}$.
To see the solver at the heart of each iteration, re-write Equation~\ref{eqn1} as Equation~\ref{eqn2}
\begin{equation}
\label{eqn3}
A x = b
\end{equation}
where 
\be
\item \(W\) is symmetric and positive definite
\item Because \(W\) is symmetric and positive definite, \(A = 
  X^T W X\) is at least positive semi-definite.
\item If the attributes are linearly independent, then \(A\) will actually be
  positive definite; else, the dependent attributes should be removed prior to
  starting the computation.

\framebox{{\bf ANDREW Any easy way to do the above? }}

\ee

\section{computations}

Step by step computations in Table~\ref{step_by_step_calc}.
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|} \hline \hline
  {\bf Name} & {\bf Description} & {\bf Type} & {\bf Code} \\ \hline \hline
  \(t_1\) & \(X \beta\) & \((N \times M) \times (M \times 1)\)  &  \\
  & & \(= N \times 1\) & \\ \hline
  \(p\) & & \(N \times 1\) & \( p = \mathrm{logit}(t1) = e^{t1}/(1 + e^{t1})\) \\ \hline
  \(t_2\) &  \(y - p\) & \(N \times 1\) & \( t_2 = \mathrm{sub}(y, p)\) \\ \hline
  \(w\) & & \(N \times 1\) & \( w = \mathrm{logit2}(t1) = e^{t1}/(1 + e^{t1})^2\) \\ \hline
  \(b\) & \(X^T (y-p)\) & \((M\times N) \times (N \times 1)\)
  & \(\forall j_{j=1}^{j=M} b_j = \) \\ 
        & & \( = M \times 1 \) & \(\mathrm{sumprod}(X_j, t_2)\) \\ \hline
  \(A\) & \(X^T W X\) & \((M \times N) \times (N \times N) \times (N \times M)\)
  & \(\forall_{j=1}^{j=M} \forall_{k=j}^{k=M} A_{j, k} = \) \\ 
  & & \(= (M \times M)\) & \(\mathrm{sumprod2}(X_j, w, X_k)\) \\ \hline
  \hline

\hline
\end{tabular}
\caption{Listing of individual steps and intermediate values}
\label{step_by_step_calc}
\end{table}

\begin{table}[hb]
\centering
\begin{tabular}{|l|l|l|l|l|} \hline \hline
  {\bf Name} & {\bf Input Type} & {\bf Output Type} & {\bf Return Value} \\ \hline \hline
  logit & Vector \(x\) & Vector \(y\) & \(y = \frac{e^x}{1 + 1^x}\) \\ \hline
  logit2 & Vector \(x\) & Vector \(y\) & \(y = \frac{e^x}{(1 + 1^x)^2}\) \\ \hline
  vvadd & Vector \(x\), Vector \(y\) & Vector \(z\) & \(z = x + y \)  \\ \hline
  vvsub & Vector \(x\), Vector \(y\) & Vector \(z\) & \(z = x - y \)  \\ \hline
  vvmul & Vector \(x\), Vector \(y\) & Vector \(z\) & \(z = x \times y \)  \\ \hline
  vvdiv & Vector \(x\), Vector \(y\) & Vector \(z\) & \(z = x / y \)  \\ \hline
  vsmul & Vector \(x\), Scalar \(y\) & Vector \(z\) & \(\forall i:~z_i = x_ \times y \)  \\ \hline
  sumprod & Vector \(x\), Vector \(y\) & Scalar \(z\) & \(z = \sum_i (x_i \times y_i)\) \\ \hline
  sumprod2 & Vector \(x\), Vector \(y\), Vector \(z\) & Scalar \(w\) &  \(w = \sum_i (x_i \times y_i \times z_i)\) \\ \hline
\hline
\end{tabular}
\caption{Necessary Operators}
\label{tbl_custom_ops}
\end{table}


\section{Details}

\subsection{Notes}

\be
\item The calculation of \(A\) is simplified by the fact that the off-diagonal
  elements of \(w\) are 0 and that it is a symmetric matrix. See last row of
  Table~\ref{step_by_step_calc}
\ee
\subsection{Clarifications needed}

\be
\item Initial guess for \(\beta\)
\ee

\section{Putting it all together}
The Q code will look like the following.

\begin{verbatim}
t1 = mvmul(X, beta)  
p = logit(t1)
w = logit2(t1)
t2 = vvsub(y, p)
for j in 1 to M do 
  b[j] = sumprod(Xj, t2)
end
for j in 1 to M do 
  for k in j to M do 
    A[j][k] = sumprod2(Xj, w, Xk)
  end
end
\end{verbatim}


\end{document}
