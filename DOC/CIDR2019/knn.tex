\newpage
\subsection{k-Nearest Neighbors}
\label{knn}

This ML workhorse doesn't need much introduction \cite{XXXX}. A key subroutine
is presented in Figure~\ref{knn_core}, where we are looking for the class labels of the
\(k\) points closest to the point \(x\) being classified. 
We assume that we have \(n\) points in \(m\)-dimensional space, represented as a
Lua table, \(T\), of \(m\) Vectors of length \(n\)

We would like to draw the
reader's attention to the ordering of operations, which are performed in column
order rather than row order. Row ordering would have meant computing the
distance from \(x\) to a point in \(T\) completely before proceeding to the next
point.
It is true that column ordering is not as intuitive as row ordering. Our claim
is that the performance gains that come from being
aware of data layout are substantial and that the programming burden is not onerous.

Given Q's lazy evaluation in chunks, note that the operation is actually
performed in batches. In other words, had we written 
{\tt d1, g1 = Q.mink(d, g, k)} and then done {\tt g1:chunk()} instead  of {\tt
g1:eval()}, 
we would have gotten the class labels of the \(k\) points
closest to \(x\) from the first \(n_C\) points of \(T\). 


Comparing our implementation with \url{http://scikit-learn.org/}, \TBC

\begin{figure}[hbtp]
\centering
\fbox{
\begin{minipage}{14 cm}
\centering
\verbatiminput{../../ML/KNN/lua/knn.lua}
\caption{core of k-nn algorithm in Lua}
\label{knn_core}
\end{minipage}
}
\end{figure}


