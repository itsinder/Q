\subsection{Exploiting Slowly Changing Summary Statistics}

In many analytical tasks, one repeatedly performs very similar operations on slowly
changing data. In these cases, 
it is not onerous to maintain (and periodically refresh) such statistics. 
Note that the fidelity demanded of these summary statistics is often low ---
they need to be only as good as the use to which they are put. For example, when
used for load balancing, they need to be good enough to guard against excessive
skew.

Similarly, 
in the process of building and tuning machine, the practitioner ends up
developing tremendous intuition about the nature of the data they work with.
These can often be used to simplify and speedup programs with relative little
pain. One of several problems that motivated this approach is as follows.


%% Examples are time series analysis to project the next 30 days
%% based on the last 90 days, anomaly detection, trend detection, \ldots 
%% In such
%% cases, summary statistics of the underlying data change slowly 
%% and 
%% Examples of such
%% statistics are average length of a web session, distribution of page views
%% across different parts of a website.  

\subsubsection{Example: The social graph}

Assume that a social graph is presented as a 
a table with 2 columns, {\tt from} and {\tt to},
A useful pre-processing step is to sort this table, with {\tt from} as the
primary key and {\tt to} as the secondary key.

Consider a simple parallelization of the sort routine. Assume that we 
``magically'' know a set of mutually exclusive and exhaustive intervals such
that each interval would get ``roughly'' the same number of elements of an input
Vector \(x\). Then a simple parallel sort consists of 
(1) partitioning \(n\) elements of \(x\) into \(n_B\) bins so that each bin has
\(n(b)\) elements
(2) sorting each bin in parallel
(3) copying each bin into the right place. Probabilistic guarantees
of the form \(P[n(b) \geq (1+\beta) \times(n/n_B)] \leq \epsilon\) 
allow us to allocate slightly more space than absoutely necessary and to 
fall back to the sequential sort in the small probability that our estimates are off.

To support this, the {\tt Q.sort} operator accepts ``hints'' in the form of
optional arguments. In this case, the hint is a Vector whose length is the
number of bins and each element is the upper bound of that bin. We report
performance numbers in Table~\ref{XXX}

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|} \hline \hline
  {\bf n} & {\bf Speedup} \\ \hline \hline
  \(2^{27}\) & X \\ \hline
  \(2^{28}\) & X \\ \hline
  \(2^{29}\) & X \\ \hline
  \(2^{30}\) & X \\ \hline
  \(2^{31}\) & X \\ \hline
  \hline
\end{tabular}
\caption{Speedup obtained with parallel sort}
\label{tbl_sort_speedup}
\end{table}

\subsection{Exploiting meta data}

To quote George Santayana, ``those who don't remember the past are condemned to
repeat it''.  \Q\ minimizes re-work in several ways
\be
\item 
Remembering basic statistics such as min, max, sum, average, whenever the
corresponding operators are invoked. 
\item the sorted-ness of a vector is
stored as one of (a) unknown (b) ascending (c) descending (d) not sorted. \Q\
uses sort heavily to simplify other operators by converting them into linear
scans. For example, consider {\tt x, y  = Q.count(z)} where \(x\) contains the
unique values of \(z\) and \(y\) the number of occurrences. If \(z\) is not
sorted, then the count operator internally (a) creates a copy, \(z'\), of \(z\)
(b) sorts \(z'\) (c) passes \(z'\) to the count sub-routine and (d) \(z'\) gets
garbage collected when the count operator returns. Of course, if \(z\) is
sorted, then this overhead is eliminated.

\ee
