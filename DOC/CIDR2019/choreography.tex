\subsection{Choreographing computations}

One of the main ways by which large data systems are optimized is minimizing the
amount of data that is moved around, as is reflected by the large body of work in
both academia and industry around optimzing compilers. 

Q takes a fundamentally different approach. It is our belief that the
choreography of computations is best left to the database programmer. This is
possible if the database programmer has (i) some understanding of the underyling
system architecture and (ii) relatively simple knobs to influence the run time
system.  In this section, we use a number of examples to illustrate this point.


\subsection{Fold}
Consider the case where we want to perform many different reductions (e.g., min,
max, sum, \ldots) over the same vector. The simple way to do this is
\begin{verbatim}
x = Q.sum(w); y = Q.min(w); z = Q.max(w)
\end{verbatim}
However, this necessitates several scans over the vector \(w\). It is more
efficient to evaluate the vector \(w\) a chunk at a time, perform all the
reductions on the chunk, store the partial results and then repeat over
successive chunks. So, we can re-write the above as 

%% TODO will have to materialize w otherwise
\begin{verbatim}
x, y, z = Q.fold({ "sum", "min", "max" }, w)
\end{verbatim}
where the fold operator is 
\begin{verbatim}
-- f_to_s is a table consisting of all reducers registered with Q
local f_to_s = require 'Q/OPERATORS/F_TO_S/lua/_f_to_s'
local function fold( fns, vec)
  local status
  -- setup reducers for each input 
  local gens = {} 
  for i, v in ipairs(fns) do gens[i] = f_to_s[v](vec) end
  -- for each chunk, evaluate reducer on that chunk
  repeat 
    for i, v in ipairs(fns) do status = gens[i]:next() end 
  until not status
  -- return results for each reducer in return_vals
  local return_vals = {}
  for i, v in ipairs(gens) do return_vals[i] = gens[i]:eval() end
  return unpack(return_vals)
end
\end{verbatim}
