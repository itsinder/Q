\section{Power to the People}

It is well accepted that large data systems benefit significantly 
from careful optimization of data movement. Optimizing compilers and query plan
rewriters aim to do this automatically.
Q takes a fundamentally different approach. It is our belief that the
choreography of computations can be left to the database programmer {\bf if} 
they have (i) some understanding of the underyling
system architecture and (ii) relatively simple knobs to influence the run time
system.  

In many analytical tasks, one repeatedly performs very similar operations on slowly
changing data. In these cases, 
it is not onerous to maintain (and periodically refresh) summary statistics that
can significantly speed up more complex operations. For example, sort is
trivially parallelized if one has a rough distribution of the keys.
Note that the fidelity demanded of these summary statistics is often low ---
they need to be only as good as the use to which they are put. 

\input{social_graph}
