\documentclass{article}
\usepackage{amsmath}

\title{A Simple Solver for Matrix Equations}
\author{Andrew Winkler, Ph.~D.}

\begin{document}

\maketitle

\section{Abstract}

We derive a solver for linear systems $Ax=b$, which works for real, complex, rational,
and certain other scalars. It is related to the Cholesky decomposition, but sidesteps creating a factorization
to directly construct the solution. We include a C implementation of the most important case, that where
real numbers are represented as double precision numbers and the matrix $A$ is symmetric and positive semi-definite.


\section{Definitions}

The adjoint of a matrix, denoted $A^*$, is equivalent to the transpose when the
matrix has real entries. The transpose is exchanging the rows and columns of the
matrix, so that $A_{i, j} = A^*_{j, i}$.

A symmetric matrix is symmetric along its diagonal, so that $A_{i, j} = A_{j,
  i}$. Notice that this is equivalent to saying that $A = A^*$.

A positive definite matrix $A$ satisfies the property that for any non-zero
vector $x$, $x^*Ax > 0$. A positive semi-definite matrix only needs to satisfy
$x^tAx \geq 0$. Notice that for any matrix $A$, we have $(A^*A) = A^* A^{**} =
A^*A$, and $x^*(A^*A)x = (Ax)^*(Ax) \geq 0$ for any $x$. So $A^*A$ is always
symmetric and positive semi-definite.

\section{Linear Equations}

We seek all $x$ for which $Ax=b$. We start by noticing that if
$Ax=b$, then also $A^*Ax=A^*b$. The converse need not be true,
but $\{x|Ax=b\} \subset \{x|A^*Ax = A^*b\}$, and in fact it's the simplest sort of subset:
it's either empty, or it's everything. To see this, note that if $A^*Ax_1 = A^*Ax_2 $,
then $$0 = A^*A(x_1-x_2)
0=(x_1-x_2)^*A^*A(x_1-x_2)=(A(x_1-x_2))^*(A(x_1-x_2)) $$ But then $A(x_1-x_2)=0
$, so $Ax_1 = Ax_2 $. Thus any solutions $x_1, x_2$ to $A^*Ax = A^*b$ must satisfy
$Ax_1 = Ax_2$, and so they are either all equal to $b$ or there is no solution.

So if we find any solution to $A^*Ax = A^*b$, we can then compare $Ax$ to $b$
and either $x$ is a solution, or there is no solution.

But recall that $A^*A$ is positive and semidefinite, and so an algorithm for
solving positive semidefinite linear equations can easily be adapted to solve
any linear equation through the conversion described above. We describe such an
algorithm in the next section.

\section{Linear Positive Equations}

Let $A$ be a symmetric and positive semidefinite matrix in $n$ dimensions. We want to find a solution to
$Ax = b$.

We write $A = A_n$ as $\begin{bmatrix}\alpha_n & a^*_{n - 1} \\ a_{n - 1} & A_{n
    - 1}\end{bmatrix}$, tagging each element with their dimension.
If $n = 1$, then all expressions involving $A_{n-1}$, $a_{n - 1}$, or $b_{n - 1]}$ are
empty sums, and therefore 0. Otherwise, $A_{n - 1}$ is $n-1$ by $n-1$, and $a_{n
- 1}$ and $b_{n - 1}$ are $n-1$ column vectors, while $\alpha_n$ is a scalar.

Since $A_n=A^*_n$, we have
$$ \begin{bmatrix}\alpha_n & a^*_{n-1} \\ a_{n - 1} & A_{n - 1}\end{bmatrix} = 
   \begin{bmatrix}\alpha_n & a_{n-1}^* \\ a_{n-1} & A_{n-1}\end{bmatrix}^* = 
   \begin{bmatrix}\alpha_n^* & a_{n-1}^* \\ a_{n-1} & A_{n-1}\end{bmatrix} $$
and it follows that $\alpha_n^*=\alpha_n$, $A_{n-1}^{*} = A_{n-1}$.

Moreover, 
$$
0 \leq \begin{bmatrix}1\\0\end{bmatrix}^*\begin{bmatrix}\alpha_n & a_{n-1}^* \\ a_{n-1} & A_{n-1}\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix}
= \begin{bmatrix}1&0\end{bmatrix}\begin{bmatrix}\alpha_n \\ a_{n-1}\end{bmatrix}
= \alpha_n
$$
There are then two cases to consider, $\alpha = 0$ and $\alpha > 0$. In either
case, we will reduce the problem to a new equation $A'x = b'$, where $A'$ in
$b'$ are in $n - 1$ dimensions, and then recursively apply the algorithm until
the problem is solved.

\subsection{Case 1}
Suppose $\alpha_n=0$. Then in fact it's also true that $a_{n-1}=0$, since for
any $\chi$ we must have
\begin{align*}
  0 &\leq \begin{bmatrix}\chi\\a_{n-1}\end{bmatrix}^*\begin{bmatrix}0 & a_{n-1}^*
  \\ a_{n-1} & A_{n-1}\end{bmatrix}\begin{bmatrix}\chi\\a_{n - 1}\end{bmatrix} \\&= \begin{bmatrix}\chi^* & a_{n-1}^*\end{bmatrix} \begin{bmatrix}a_{n-1}^*a_{n-1}\\a_{n-1}\chi+A_{n-1}a_{n-1}\end{bmatrix}
\\&= \chi^*a_{n-1}^*a_{n-1} + a_{n-1}^*a_{n-1}\chi + a_{n-1}^*A_{n-1}a_{n-1}
\end{align*}

Notice that the first inequality relies on the assumption that $A_n$ was
positive semidefinite. To see that this equation forces $a_{n-1}$ to be zero,
notice that this is the equation of a line. The only lines that have no negative
values are horizontal, so the slope $2a_{n-1}^*a_{n-1}$ must be zero, which means $a_{n-1}$ must
be zero.

Our equation then simplifies to 
$
\begin{bmatrix}0 & 0 \\ 0 & A_{n-1}\end{bmatrix}\begin{bmatrix}\chi_n\\x_{n-1}\end{bmatrix} =\begin{bmatrix}\beta_n\\b_{n-1}\end{bmatrix}
$, or $0=\beta_n$, and
$A_{n-1}x_{n-1}=b_{n-1} $. The first equation tells us that there can be no solution unless
$\beta_n=0$. If, however, it is, then any solution $x_{n-1}$ of the
second equation, which we note, satisfies the same properties we required of 
$A_n$, can be combined with any number $z_n$ into a solution,
$A_n\begin{bmatrix}z_n\\x_{n-1}\end{bmatrix} = b_n$

In particular, we can always take $z_n$ to be 0.

\subsection{Case 2}
Suppose $\alpha_n>0$. Then
\begin{align*}
&\begin{bmatrix}\alpha_n & a_{n-1}^* \\ a_n & A_{n-1}\end{bmatrix}\begin{bmatrix}\chi_n\\x_{n-1}\end{bmatrix}=\begin{bmatrix}\beta_n\\b_{n-1}\end{bmatrix}
\\\iff&
\alpha_n\chi_n + a_{n-1}^*x_{n-1} = \beta_n &\text{ and }\hspace{2em}&
a_n\chi_n + A_{n-1}x_{n-1} = b_{n-1}
\\\iff&
\alpha_n\chi_n= \beta_n - a_{n-1}^*x_{n-1}  &\text{ and }\hspace{2em}&
a_n\chi_n + A_{n-1}x_{n-1} = b_{n-1}
\\\iff&
\chi_n= \alpha_n^{-1}(\beta_n - a_{n-1}^*x_{n-1}) &\text{ and }\hspace{2em}&
a_n\alpha_n^{-1}(\beta_n - a_{n-1}^*x_{n-1}) + A_{n-1}x_{n-1} = b_{n-1}
\\\iff&
\chi_n= \alpha_n^{-1}(\beta_n - a_{n-1}^*x_{n-1}) &\text{ and }\hspace{2em}&
a_n\alpha_n^{-1}(- a_{n-1}^*x_{n-1}) + A_{n-1}x_{n-1} = b_{n-1}-a\alpha_n^{-1}\beta_n
\\\iff&
\chi_n= \alpha_n^{-1}(\beta_n - a_{n-1}^*x_{n-1}) &\text{ and }\hspace{2em}&
(A_{n-1} - a_n\alpha_n^{-1}a_{n-1}^*)x_{n-1} = b_{n-1}-a_{n-1}\alpha_n^{-1}\beta_n
\end{align*}

  
So if we can solve the second equation, the first equation shows us how to convert it into a solution
of the original equation. Notice that if 
$b_n =0 $, then so is the derived right hand side; a homogeneous problem generates
a homogeneous subproblem.

But this second equation is again of the same form, because
\begin{gather}{}(A_{n-1} - a_{n-1}\alpha_n^{-1}a_{n-1}^*)^* = 
A_{n-1}^{*} - a_{n-1}^{**}\alpha_n^{-1}a_{n-1}^* = 
A_{n-1} - a_{n-1}\alpha_n^{-1}a_{n-1}^* 
\end{gather}
and moreover, for any vector $x$ we have
\begin{gather}{}
0 \leq \begin{bmatrix}-\alpha_n^{-1}a_{n-1}^*x\\x\end{bmatrix}^*\begin{bmatrix}\alpha_n & a_{n-1}^* \\ a_{n-1} & A_{n-1}\end{bmatrix}\begin{bmatrix}-\alpha_n^{-1}a_{n-1}^*x\\x\end{bmatrix}
= 
x^*(A_{n-1} - a_{n-1}\alpha_n^{-1}a_{n-1}^*)x
\end{gather} So the derived submatrix is nonnegative, and positive if the original matrix is.

As in either case we have reduced to either the base case of dimension one or to a similar problem in one dimension smaller, the solution, whether it exists or
not, and whether or not multiple solutions exist, is complete.

\section{Preliminaries}

We consider the linear system $Ax=b$. If your only interest is in the real numbers,
you can skip the rest of this section with the notation that $A^*$ means the transpose of the matrix $A$. In particular a number, which can be seen as a 1x1 matrix, is its own transpose.

The algorithm we describe works perfectly well for finite dimensional Hilbert spaces over various fields, to be described. However, since the method implicitly constructs a basis for the vector space, for ease of exposition and with no real loss of generality, we assume already chosen an orthonormal basis, with respect to which A is a matrix, and x and b are column vectors.

We can view rational numbers as lying in the larger field of real numbers; likewise the real numbers in the complex numbers, and the complex numbers in the quaternions. A quaternion satisfies $(a + bi +cj + dk)^* = (a -bi -cj -dk)$, from which it is immediate that $q^*q = a^2 + b^2 + c^2 + d^2 >= 0$. Moreover if $q^*q =0$ then $q =0$.
$q* = q$ if and only if $q$ is real.

The algorithm assumes only that the field is some subfield of the quaternions, which is closed under this conjugation operation, which is the same thing as saying that if $\alpha$ is in the field, so is $\alpha^*\alpha$, or equivalently $\alpha + \alpha^*$. This is of course true of the rationals, the reals, any subfield of the reals, the complexes, and the quaternions.

Recall that the adjoint of a matrix $A$, denoted $A^*$, is the transpose of the matrix obtained from $A$ by replacing each number $a_{ij}$ by its complex/quaternion conjugate $a_{ij}^*$.


\section{After-thought}
This section can be safely skipped by anyone whose interest is confined to
the real numbers, the complex numbers, the rational numbers, or even the quaternions.

The method works provided the scalars are taken from any subfield of the
quaternions which is closed under the * operation. More precisely, we really
need $\alpha^* = \alpha$ if and only if $\alpha$ is real, that $\alpha^*\alpha >= 0$, and that 
$\alpha^*\alpha = 0$ only if $\alpha=0$.

But the method works, at least in some cases, even if the field is not *-closed.We can take the *-closure of any subfield of the quaternions, by intersecting all *-closed subfields containing it. Since the quaternions are *-closed, that *-closure is a *-closed subfield of the quaternions. We can then find all solutions composed of vectors having entries in the closure; it remains to determine
which of those solutions, if any, lie in the original field. In the case where
$A$ is positive, the solution is unique, and a solution whose entries lie in the original field does exist, so must coincide with the solution obtained by this method, which then must necessarily also have entries lying in the original field.

\section{Implementation Notes}
At each level of recursion, we only need to retain
$a$,
$\alpha$
and
$\beta$, allowing us to overwrite
$A^{'}$
with $(A^{'} - a\alpha^{-1}a^*)$
and
$b^{'}$ with
$b^{'}-a\alpha^{-1}\beta$
.

We conclude with a C-language implementation of a solver for the positive semi-definite case, where
$x^*Ax >=0 $ for all $x$.

\begin{verbatim}
$ cat positive_solver.h 
extern
double * positive_solver(double ** A, double * b, int n);

$ cat positive_solver.c
/* 
 * Andrew Winkler

This code solves the equation Ax=b, where A is a positive
semi-definite matrix, so that x^t A x >= 0 for all x,
provided a solution exists. If A is positive definite,
it will be an isomorphism, but in general conditions
must be imposed on b, for it to lie in the range of A.

It has the virtue of dramatic simplicity - there's no need
to explicitly construct the Cholesky decomposition, no need
to do the explicit back-substitutions.  Yet it's essentially
equivalent to that more labored approach, so its
performance/stability/memory, etc. should be at least as good.

There are two kinds of checks, both of which are disabled.
This means that a solution will be generated; it is up to
the caller to verify that the equation Ax=b is satisfied to
the desired precision. If it does not, then no solution exists.

The first kind of check is that Avec is 0 whenever A[0][0]
is 0, which will always be true if A is positive semi-definite,
but which could become false because of any small perturbation
caused by roundoff error upstream in the computation of A.
The second kind of check is that b[0] is 0 whenever A[0][0] is 0,
which is necessary for b to lie in the range of A. The checks are
disabled because the code does not deal with real numbers,
but rather floating point numbers; it's up to the user to decide
what is "close enough" to zero.

*/

#include <stdlib.h>
#include <stdio.h>
#include "positive_solver.h"

void _consistency_checker(double * v, int n) {
    for (int i=0; i<n; i++) {
        if ( v[i] != 0.0 ) exit(-1);
    }
}

void _positive_solver(
    double ** A, 
    double * b, 
    double * x, 
    int n
    ) 
{
  if (n < 1) exit(-1);

  if (n == 1) {
    if (A[0][0] == 0.0) {
        /* _consistency_checker(b, 1); */
        x[0] = 0.0;
        return;
    }
    x[0] = b[0] / A[0][0];
    return;
  }

  double * bvec = b + 1;
  double * Avec = A[0] + 1;
  double ** Asub = A + 1;
  double * xvec = x + 1;

  int m = n -1;

  if (A[0][0] == 0.0) {
      /*
      _consistency_checker(b, 1);
      _consistency_checker(Avec, m);
      */
  } else {
      for(int j=0; j < m; j++){
        bvec[j] -= Avec[j] * b[0] / A[0][0];
        for(int i=0; i < m - j; i++)
          Asub[i][j] -= Avec[i] * Avec[i+j] / A[0][0];
      }
  }

  _positive_solver(Asub, bvec, xvec, m);

  if (A[0][0] == 0.0) {
      x[0] = 0.0;
      return;
  }

  double p = 0; for(int k=0; k<m; k++) p += Avec[k] * xvec[k];

  x[0] = (b[0] - p) / A[0][0];

  return;
}

#include <malloc.h>
double * positive_solver(
    double ** A, 
    double * b, 
    int n
    ) 
{
  double * x = (double *) malloc(n * sizeof(double));
  _positive_solver(A, b, x, n);
  return x;
}

\end{verbatim}
\end{document}
